{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "with open('parameters.json', 'r') as parameters:\n",
    "    data = json.load(parameters)\n",
    "\n",
    "\n",
    "for name in data.keys():\n",
    "    data[name] = pd.read_csv(data[name]['path_to_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper function to apply methods for each data set in data\n",
    "def aplpy_to_all(func, data):\n",
    "    for name in data.keys():\n",
    "        func(data[name], name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, each benchmark tells us number of failed and successfull actions. Let us look whether there is some parameter value that led to more than 0 fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_check(df, name):\n",
    "\n",
    "    if df['Fail'].sum() == 0:\n",
    "        print(f'No values of parameter {name} led to failures')\n",
    "        return\n",
    "\n",
    "    def check_failures(row, name):\n",
    "        if int(row['Fail']) > 0:\n",
    "            print(f'Parameter {name} led to {row['Fail']} number of failures')\n",
    "    \n",
    "    df.apply(lambda row: check_failures(row, name), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No values of parameter batchSize led to failures\n",
      "No values of parameter maxBlockCountToStore led to failures\n",
      "No values of parameter maxPropagationBurstLatency led to failures\n",
      "No values of parameter pullInterval led to failures\n"
     ]
    }
   ],
   "source": [
    "aplpy_to_all(apply_check, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we can be sure that there is no parameter value that led to extraordinary benchmark result but failed 99% of the time. Let us now look at the statistics for throughput in each data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_basic_stat(df, name):\n",
    "    t = 'Throughput'\n",
    "    print(f'Analyzing {name}:')\n",
    "    print(\n",
    "        f'Average {t}: {df[t].mean()}',\n",
    "        f'Median {t}: {df[t].median()}',\n",
    "        f'Min {t}: {df[t].min()}',\n",
    "        f'Max {t}: {df[t].max()}', sep='\\n'\n",
    "    )\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing batchSize:\n",
      "Average Throughput: 520.8777777777779\n",
      "Median Throughput: 523.2\n",
      "Min Throughput: 487.7\n",
      "Max Throughput: 562.2\n",
      " \n",
      "Analyzing maxBlockCountToStore:\n",
      "Average Throughput: 520.8\n",
      "Median Throughput: 528.4\n",
      "Min Throughput: 489.0\n",
      "Max Throughput: 546.7\n",
      " \n",
      "Analyzing maxPropagationBurstLatency:\n",
      "Average Throughput: 499.62727272727267\n",
      "Median Throughput: 504.7\n",
      "Min Throughput: 457.8\n",
      "Max Throughput: 530.2\n",
      " \n",
      "Analyzing pullInterval:\n",
      "Average Throughput: 497.39090909090913\n",
      "Median Throughput: 504.8\n",
      "Min Throughput: 458.5\n",
      "Max Throughput: 536.5\n",
      " \n"
     ]
    }
   ],
   "source": [
    "aplpy_to_all(show_basic_stat, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's look at dependence between parameter value on throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read CSV files\n",
    "data1 = pd.read_csv('benchmark_result/batchSize_reports.csv')\n",
    "data2 = pd.read_csv('benchmark_result/maxBlockCountToStore_reports.csv')\n",
    "data3 = pd.read_csv('benchmark_result/maxPropagationBurstLatency_reports.csv')\n",
    "data4 = pd.read_csv('benchmark_result/pullInterval_reports.csv')\n",
    "\n",
    "# Generate some example standard deviation values\n",
    "# These should ideally be part of your CSV data or calculated from your data\n",
    "data1['std'] = data1['Throughput'] * 0.1\n",
    "data2['std'] = data2['Throughput'] * 0.1\n",
    "data3['std'] = data3['Throughput'] * 0.1\n",
    "data4['std'] = data4['Throughput'] * 0.1\n",
    "\n",
    "# Create a figure and a 2x2 grid of subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(7, 5))\n",
    "\n",
    "# Plot the first dataset with error region\n",
    "axs[0, 0].plot(data1['batchSize'], data1['Throughput'], color='blue')\n",
    "axs[0, 0].fill_between(data1['batchSize'], data1['Throughput'] - data1['std'], data1['Throughput'] + data1['std'], color='blue', alpha=0.2)\n",
    "axs[0, 0].set_xlabel('Batch Size')\n",
    "axs[0, 0].set_ylabel('Throughput')\n",
    "\n",
    "# Plot the second dataset with error region\n",
    "axs[0, 1].plot(data2['maxBlockCountToStore'], data2['Throughput'], color='red')\n",
    "axs[0, 1].fill_between(data2['maxBlockCountToStore'], data2['Throughput'] - data2['std'], data2['Throughput'] + data2['std'], color='red', alpha=0.2)\n",
    "axs[0, 1].set_xlabel('maxBlockCountToStore')\n",
    "axs[0, 1].set_ylabel('Throughput')\n",
    "\n",
    "# Plot the third dataset with error region\n",
    "axs[1, 0].plot(data3['maxPropagationBurstLatency'], data3['Throughput'], color='green')\n",
    "axs[1, 0].fill_between(data3['maxPropagationBurstLatency'], data3['Throughput'] - data3['std'], data3['Throughput'] + data3['std'], color='green', alpha=0.2)\n",
    "axs[1, 0].set_xlabel('maxPropagationBurstLatency')\n",
    "axs[1, 0].set_ylabel('Throughput')\n",
    "\n",
    "# Plot the fourth dataset with error region\n",
    "axs[1, 1].plot(data4['pullInterval'], data4['Throughput'], color='purple')\n",
    "axs[1, 1].fill_between(data4['pullInterval'], data4['Throughput'] - data4['std'], data4['Throughput'] + data4['std'], color='purple', alpha=0.2)\n",
    "axs[1, 1].set_xlabel('pullInterval')\n",
    "axs[1, 1].set_ylabel('Throughput')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure as a PGF file\n",
    "plt.savefig('line_plots_with_error_regions.pgf', bbox_inches=\"tight\")\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
